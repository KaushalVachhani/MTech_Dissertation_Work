{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Processed] Transformer Architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKX3lg8mB13n"
      },
      "source": [
        "### Connect to drive and setup training file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aeXVy-GB1Tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62b4f09-3927-416e-b8e5-4c91fb170482"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51kSQ9h5B5id"
      },
      "source": [
        "### Importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nH_PLvO9qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fa48a3-6a51-4cb8-e1c5-46edb13d4e56"
      },
      "source": [
        "installed = False\n",
        "\n",
        "if not installed:\n",
        "    !rm -rf indic_nlp_library indic_nlp_resources >> /dev/null\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_resources.git\" --quiet\n",
        "    !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\" --quiet\n",
        "    !pip install -r \"./indic_nlp_library/requirements.txt\" >> /dev/null\n",
        "    !pip install indic-nlp-library >> /dev/null\n",
        "    !pip install Morfessor >> /dev/null"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq-ieosa-1dD"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from indicnlp import loader\n",
        "from indicnlp import common\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import indicnlp\n",
        "import random\n",
        "import torch\n",
        "import nltk\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import sys\n",
        "import csv\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JYjFxvMJuZj",
        "outputId": "ae591e6d-5258-400f-d7af-bd9359d007be"
      },
      "source": [
        "nltk.download('punkt', quiet=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIQwzHuI8J6T"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCoO8TRKHsOf"
      },
      "source": [
        "INDIC_NLP_LIB_HOME   =  \"./indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES  =  \"./indic_nlp_resources\"\n",
        "\n",
        "# Add indicnlp to system path:\n",
        "sys.path.append(INDIC_NLP_LIB_HOME)\n",
        "\n",
        "# Point the indicnlp resources:\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzifVhv7_rXP"
      },
      "source": [
        "hi_word2id = {}\n",
        "hi_id2word = {}\n",
        "en_word2id = {}\n",
        "en_id2word = {}\n",
        "\n",
        "hi_word2freq = {}\n",
        "en_word2freq = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnG-V_LQAa7r"
      },
      "source": [
        "### Adding Start Word and Stop Word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD7Gt7oYs9mJ"
      },
      "source": [
        "# Adding start, stop words in hindi vocab:\n",
        "hi_word2id.update({'__<<init>>__': 0})\n",
        "hi_id2word.update({0: '__<<init>>__'})\n",
        "hi_word2id.update({'__<<stop>>__': 1})\n",
        "hi_id2word.update({1: '__<<stop>>__'})\n",
        "hi_word2id.update({'__<<unknown>>__': 2})\n",
        "hi_id2word.update({2: '__<<unknown>>__'})\n",
        "hi_word2id.update({'__<<padding>>__': 3})\n",
        "hi_id2word.update({3: '__<<padding>>__'})\n",
        "\n",
        "# Adding start, stop words in english vocab:\n",
        "en_word2id.update({'__<<init>>__': 0})\n",
        "en_id2word.update({0: '__<<init>>__'})\n",
        "en_word2id.update({'__<<stop>>__': 1})\n",
        "en_id2word.update({1: '__<<stop>>__'})\n",
        "en_word2id.update({'__<<unknown>>__': 2})\n",
        "en_id2word.update({2: '__<<unknown>>__'})\n",
        "en_word2id.update({'__<<padding>>__': 3})\n",
        "en_id2word.update({3: '__<<padding>>__'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eHsS_tXBeSq"
      },
      "source": [
        "### Loading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpxJmZtIEtRi"
      },
      "source": [
        "# dataset = []\n",
        "# with open('./train.csv', 'r') as file:\n",
        "#     dataset = np.array([[r[1], r[2]] for r in csv.reader(file)])[1::]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKGYDSBHl9Qg",
        "outputId": "8b894c3d-8bce-4f38-ee00-18908266d4b6"
      },
      "source": [
        "hi_dataset = []\n",
        "with open('./drive/MyDrive/processed.hi', encoding='utf8') as file:\n",
        "    hi_dataset = [s.strip('\\n').replace('  ', ' ') for s in file.readlines()]\n",
        "\n",
        "hi_dataset[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['आदि में परमेश्वर ने आकाश और पृथ्वी की सृष्टि की ।',\n",
              " 'और पृथ्वी बेडौल और सुनसान पड़ी थी ; और गहरे जल के ऊपर अन्धियारा था : तथा परमेश्वर का आत्मा जल के ऊपर मण्डलाता था ।',\n",
              " 'तब परमेश्वर ने कहा , उजियाला हो : तो उजियाला हो गया ।',\n",
              " 'और परमेश्वर ने उजियाले को देखा कि अच्छा है ; और परमेश्वर ने उजियाले को अन्धियारे से अलग किया ।',\n",
              " 'और परमेश्वर ने उजियाले को दिन और अन्धियारे को रात कहा । तथा सांझ हुई फिर भोर हुआ । इस प्रकार पहिला दिन हो गया । ।',\n",
              " 'फिर परमेश्वर ने कहा , जल के बीच एक ऐसा अन्तर हो कि जल दो भाग हो जाए ।',\n",
              " 'तब परमेश्वर ने एक अन्तर करके उसके नीचे के जल और उसके ऊपर के जल को अलग अलग किया ; और वैसा ही हो गया ।',\n",
              " 'और परमेश्वर ने उस अन्तर को आकाश कहा । तथा सांझ हुई फिर भोर हुआ । इस प्रकार दूसरा दिन हो गया । ।',\n",
              " 'फिर परमेश्वर ने कहा , आकाश के नीचे का जल एक स्थान में इकट्ठा हो जाए और सूखी भूमि दिखाई दे ; और वैसा ही हो गया ।',\n",
              " 'और परमेश्वर ने सूखी भूमि को पृथ्वी कहा ; तथा जो जल इकट्ठा हुआ उसको उस ने समुद्र कहा : और परमेश्वर ने देखा कि अच्छा है ।']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grAvQ8EdmceO",
        "outputId": "f70c24d2-06d0-4080-fe6d-6b7bb59b6283"
      },
      "source": [
        "en_dataset = []\n",
        "with open('./drive/MyDrive/processed.en', encoding='utf8') as file:\n",
        "    en_dataset = [s.strip('\\n').replace('  ', ' ') for s in file.readlines()]\n",
        "\n",
        "en_dataset[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In the beginning God created the heaven and the earth .',\n",
              " 'And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters .',\n",
              " 'And God said , Let there be light : and there was light .',\n",
              " 'And God saw the light , that it was good : and God divided the light from the darkness .',\n",
              " 'And God called the light Day , and the darkness he called Night . And the evening and the morning were the first day .',\n",
              " 'And God said , Let there be a firmament in the midst of the waters , and let it divide the waters from the waters .',\n",
              " 'And God made the firmament , and divided the waters which were under the firmament from the waters which were above the firmament : and it was so .',\n",
              " 'And God called the firmament Heaven . And the evening and the morning were the second day .',\n",
              " 'And God said , Let the waters under the heaven be gathered together unto one place , and let the dry land appear : and it was so .',\n",
              " 'And God called the dry land Earth ; and the gathering together of the waters called he Seas : and God saw that it was good .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhXwsfCmhhaL"
      },
      "source": [
        "hi_ds = []\n",
        "en_ds = []\n",
        "for i in range(len(hi_dataset)):\n",
        "    if len(indic_tokenize.trivial_tokenize(hi_dataset[i])) > 80:\n",
        "        pass\n",
        "    else:\n",
        "        en_ds += [en_dataset[i]]\n",
        "        hi_ds += [hi_dataset[i]]\n",
        "    \n",
        "hi_dataset = hi_ds\n",
        "en_dataset = en_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm4cCthsia8O",
        "outputId": "4f5fc4c8-9c7f-435c-d352-3f5e797ac9f0"
      },
      "source": [
        "len(hi_dataset), len(en_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(511725, 511725)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2yMjTEas9mL"
      },
      "source": [
        "hi_word_seq = []\n",
        "hi_counter = 4\n",
        "for row in hi_dataset:\n",
        "\n",
        "    # Hindi Sentences:\n",
        "    temp = indic_tokenize.trivial_tokenize(row)\n",
        "    hi_word_seq += [temp]\n",
        "    \n",
        "    for word in temp:\n",
        "        if word not in hi_word2id.keys():\n",
        "            hi_word2id.update({word: hi_counter})\n",
        "            hi_id2word.update({hi_counter: word})\n",
        "            hi_counter += 1\n",
        "\n",
        "en_word_seq = []\n",
        "en_counter = 4\n",
        "for row in en_dataset:\n",
        "\n",
        "    # English Sentences:\n",
        "    temp = nltk.word_tokenize(row)\n",
        "    en_word_seq += [temp]\n",
        "\n",
        "    for word in temp:\n",
        "        if word not in en_word2id.keys():\n",
        "            en_word2id.update({word: en_counter})\n",
        "            en_id2word.update({en_counter: word})\n",
        "            en_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2hrlAkwLlj6"
      },
      "source": [
        "hi_max = max([len(l) for l in hi_word_seq])\n",
        "en_max = max([len(l) for l in en_word_seq])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOhUqEvXMvtY",
        "outputId": "fa76aa62-2dda-4a0d-fbae-a0f3d683700a"
      },
      "source": [
        "print('Hi-Vocabulary Size:', len(hi_id2word))\n",
        "print('En-Vocabulary Size:', len(en_id2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi-Vocabulary Size: 95041\n",
            "En-Vocabulary Size: 90667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMwjYh9gPmw"
      },
      "source": [
        "def get_indices_seq(seq, vocab):\n",
        "    \n",
        "    temp = []\n",
        "    for word in seq:\n",
        "        if word in vocab.keys():\n",
        "            temp += [vocab[word]]\n",
        "        else:\n",
        "            temp += [vocab['__<<unknown>>__']]\n",
        "\n",
        "    seq = torch.tensor(\n",
        "        [vocab['__<<init>>__']] + temp + [vocab['__<<stop>>__']]\n",
        "    )\n",
        "\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTf-yPHcjAt8"
      },
      "source": [
        "def get_word_seq(seq, vocab):\n",
        "\n",
        "    temp = []\n",
        "    for index in seq:\n",
        "        temp += [vocab[int(index)]]\n",
        "\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwnoQeKlvrGZ"
      },
      "source": [
        "train_seq_pairs = []\n",
        "\n",
        "for sent_id in range(len(hi_word_seq)):\n",
        "    train_seq_pairs += [[\n",
        "        get_indices_seq(hi_word_seq[sent_id], hi_word2id).to(device),\n",
        "        get_indices_seq(en_word_seq[sent_id], en_word2id).to(device) \n",
        "    ]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USbsIpwENlZv"
      },
      "source": [
        "train_seq_pairs_sorted = sorted(train_seq_pairs, key=lambda x: len(x[0]) + len(x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Z-3ATydlzg"
      },
      "source": [
        "BATCH_SIZE = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EmIFRETAQcQ"
      },
      "source": [
        "input_batches = []\n",
        "output_batches = []\n",
        "for i in range(0, len(train_seq_pairs_sorted) - (len(train_seq_pairs_sorted) % BATCH_SIZE), BATCH_SIZE):\n",
        "\n",
        "    hi_indices_sequences = [pair[0] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "    en_indices_sequences = [pair[1] for pair in train_seq_pairs_sorted[i:i+BATCH_SIZE]]\n",
        "\n",
        "    input_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            hi_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    output_batches += [\n",
        "        torch.nn.utils.rnn.pad_sequence(\n",
        "            en_indices_sequences, \n",
        "            batch_first=False, \n",
        "            padding_value=3\n",
        "        )\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uprmqlzE-ABZ",
        "outputId": "e06696fa-c701-4a15-aee7-fe17a59b98e1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 06:20:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    38W / 300W |   2069MiB / 16160MiB |     27%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATRFtjbPyE0"
      },
      "source": [
        "### Transformer Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR5tFQacK7zL"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        \n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-HTwqKdLTmi"
      },
      "source": [
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1).to(device)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNWqhMLGLZIl"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into N heads\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * N * sl * d_model\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ieKmArLcc7"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "    \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCmArZRPZce6"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTrADzPuZfug"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 200, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        pe = Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        x = x + pe\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BOzUfltL8YK"
      },
      "source": [
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz7VUDvPZrZv"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfAAn-U5ZuXe"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgL-cubLrAV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YlqikIiLuSV"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeXqPeHwLwxU"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        #print(\"DECODER\")\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76rYwzrMNWPR"
      },
      "source": [
        "opt = {\n",
        "    'SGDR': True,\n",
        "    'epochs': 20,\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'batchsize': 32,\n",
        "    'lr': 3e-4,\n",
        "    'k': 5,\n",
        "    'max_len': 100\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajbsa3ClLzNu"
      },
      "source": [
        "def get_model(opt, src_vocab, trg_vocab):\n",
        "    \n",
        "    assert opt[\"d_model\"] % opt[\"heads\"] == 0\n",
        "    assert opt[\"dropout\"] < 1\n",
        "\n",
        "    model = Transformer(src_vocab, trg_vocab, opt[\"d_model\"], opt[\"n_layers\"], opt[\"heads\"], opt[\"dropout\"])\n",
        "       \n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCFHy0bgYxq9"
      },
      "source": [
        "model = get_model(opt, len(hi_word2id), len(en_word2id)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmC3VH6Shoqm"
      },
      "source": [
        "def nopeak_mask(size):\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    return np_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5zdK42ohE36"
      },
      "source": [
        "def create_masks(src, trg):\n",
        "    \n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != en_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size).to(device)\n",
        "        trg_mask = trg_mask & np_mask\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjbQPHbGiWlF"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=opt['lr'], betas=(0.9, 0.98), eps=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8DcnNgTzJY",
        "outputId": "9ddf5282-8487-4a07-8d51-8285ebcf9b40"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 06:20:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    38W / 300W |   2785MiB / 16160MiB |     15%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dYFnTvR85qI"
      },
      "source": [
        "torch.save(model.state_dict(), 'drive/MyDrive/processed_transformer_model')\n",
        "torch.save(optimizer.state_dict(), 'drive/MyDrive/processed_transformer_optim')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4_a7Yhq9Mo2"
      },
      "source": [
        "model.load_state_dict(torch.load('drive/MyDrive/processed_transformer_model_9'))\n",
        "optimizer.load_state_dict(torch.load('drive/MyDrive/processed_transformer_optim_9'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SntL7zRY90u"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(10, 20, 1):\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    indices = list(range(len(input_batches)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    tic = time.time()\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    minibatch_X = input_batches[0]\n",
        "    minibatch_Y = output_batches[0]\n",
        "\n",
        "    for i in indices:\n",
        "\n",
        "        minibatch_X = input_batches[i].T\n",
        "        minibatch_Y = output_batches[i].T\n",
        "\n",
        "        src = minibatch_X\n",
        "        trg = minibatch_Y\n",
        "\n",
        "        # print('SRC SHAPE:', src.shape)\n",
        "        # print('TRG SHAPE:', trg.shape)\n",
        "\n",
        "        trg_input = trg[:, :-1]\n",
        "        # trg_input = trg[:, :]\n",
        "\n",
        "        src_mask, trg_mask = create_masks(src, trg_input)\n",
        "        preds = model(src, trg_input, src_mask, trg_mask)\n",
        "        \n",
        "        # print(src.shape)\n",
        "        # print(trg.shape)\n",
        "        # print(preds.shape)\n",
        "\n",
        "        # ys = trg[:, :].contiguous().view(-1)\n",
        "        ys = trg[:, 1:].contiguous().view(-1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=en_word2id['__<<padding>>__'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        counter += 1\n",
        "\n",
        "        if counter % 200 == 0:\n",
        "            print('{}/{} Batches Processed.'.format(counter, len(input_batches)))\n",
        "\n",
        "    toc = time.time()\n",
        "\n",
        "    # Save Weights:\n",
        "    torch.save(model.state_dict(), 'drive/MyDrive/processed_transformer_model_{}'.format(epoch))\n",
        "    torch.save(optimizer.state_dict(), 'drive/MyDrive/processed_transformer_optim_{}'.format(epoch))\n",
        "    \n",
        "    print('Epoch {}, Total Loss: {}, Total Time For This Epoch: {}'.format(epoch, total_loss, toc - tic))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgedivMBHDnk"
      },
      "source": [
        "def init_vars(src, model):\n",
        "    \n",
        "    # src: indices sequence (may have padding)\n",
        "    # shape: (1, 1, src_len)\n",
        "\n",
        "    init_tok = en_word2id['__<<init>>__']\n",
        "    # start token\n",
        "    # shape: scalar\n",
        "\n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "    # mask padding sequences:\n",
        "    # shape: (1, 1, src_len)\n",
        "    # \n",
        "    # =======================================\n",
        "    # Description:\n",
        "    # =======================================\n",
        "    # \n",
        "    # if src: [[[w1, w2, <p>, <p>, <p>]]] then src_mask: [[[True, True]]]\n",
        "\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "    # encoder outputs:\n",
        "    # 512 dimensional vector for each word:\n",
        "    # shape: (1, src_len, 512)\n",
        "\n",
        "    outputs = torch.LongTensor([[init_tok]]).to(device)\n",
        "    # initial output for target sentence:\n",
        "    # eg. [__<<init>>__]\n",
        "    # shape: (1, 1)\n",
        "\n",
        "    trg_mask = nopeak_mask(1)\n",
        "    # initial output mask:\n",
        "    # shape: (1, 1, 1)\n",
        "\n",
        "    out = model.out(model.decoder(outputs, e_output, src_mask, trg_mask))\n",
        "    # model output:\n",
        "    # shape: (1, 1, target_vocab)\n",
        "\n",
        "    out = F.softmax(out, dim=-1)\n",
        "    # model normalized output:\n",
        "    # shape: (1, 1, target_vocab)\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(opt['k'])\n",
        "    # probs: k topmost probabilities\n",
        "    # shape: (1, K)\n",
        "\n",
        "    # ix: k topmost indices\n",
        "    # shape: (1, K)\n",
        "\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "    # log scores: score of the sequence so far.\n",
        "    # shape: (1, K)\n",
        "\n",
        "    outputs = torch.zeros(opt[\"k\"], opt[\"max_len\"]).long().to(device)\n",
        "    # buffer for top k outputs: \n",
        "    # shape: (K, max_len)\n",
        "\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "    # set first word = '__<<init>>__' and predict next word:\n",
        "    # shape: (K, max_len)\n",
        "    \n",
        "    e_outputs = torch.zeros(opt[\"k\"], e_output.size(-2), e_output.size(-1)).to(device)\n",
        "    # allocate tensor for encoder outputs\n",
        "    # shape: (K, src_len, 512)\n",
        "\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "    # broadcast encoder outputs for each sequences\n",
        "    # shape(K, src_len, 512)\n",
        "\n",
        "    return outputs, e_outputs, log_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEHpqPUTHJ4o"
      },
      "source": [
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    \n",
        "    # outputs: (K, max_len)\n",
        "    # out: (K, i, target_vocab)\n",
        "    # log_scores: (1, K)\n",
        "    # i: current length so far.\n",
        "    # k: beam width.\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    # probs: top-k predictions for each of top k previous probabilities\n",
        "    # shape: (k, k)\n",
        "\n",
        "    # ix: top-k indices for each of top k previous probabilities\n",
        "    # shape: (k, k)\n",
        "\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0, 1)\n",
        "    # calculating log_probabilities:\n",
        "    # shape: (k, k)\n",
        "    \n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "    # k_probs shape: (5,)\n",
        "    # k_ix shape: (5,)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    # outputs: (K, max_len)\n",
        "    # log_scores: (1, K)\n",
        "    \n",
        "    return outputs, log_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbkndXROHNXn"
      },
      "source": [
        "def beam_search(src, model, opt):\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model)\n",
        "    # initialize the generation of sentence and allocate buffers\n",
        "    # outputs: (K, max_len)\n",
        "    # e_outputs: (K, seq_len, 512)\n",
        "    # log_scores: (1, K)\n",
        "\n",
        "    eos_tok = en_word2id['__<<stop>>__']\n",
        "    # target '__<<stop>>__' token:\n",
        "    # shape: scalar\n",
        "\n",
        "    src_mask = (src != hi_word2id['__<<padding>>__']).unsqueeze(-2)\n",
        "    # mask padding sequences:\n",
        "    # shape: (1, 1, src_len)\n",
        "    # \n",
        "    # =======================================\n",
        "    # Description:\n",
        "    # =======================================\n",
        "    # \n",
        "    # if src: [[[w1, w2, <p>, <p>, <p>]]] then src_mask: [[[True, True]]]\n",
        "\n",
        "    ind = None\n",
        "    for i in range(2, opt[\"max_len\"]):\n",
        "    \n",
        "        trg_mask = nopeak_mask(i)\n",
        "        # Masking the target upto current length:\n",
        "        # shape: (1, i, i) where i represents output length so far.\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i], e_outputs, src_mask, trg_mask))\n",
        "        # out: (K, i, dict) where i represents output length so far.\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        # softmax: (K, i, dict) where i represents output length so far.\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, opt[\"k\"])\n",
        "        # finding next K best words:\n",
        "        # outputs: (K, max_len)\n",
        "        # log_scores: (1, K)\n",
        "\n",
        "        ones = (outputs.long()==eos_tok).nonzero()\n",
        "\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if int(sentence_lengths[i]) == 0:\n",
        "                sentence_lengths[i] = vec[1]\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == opt[\"k\"]:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "\n",
        "    # print(num_finished_sentences)\n",
        "\n",
        "    if ind is None:\n",
        "        length = (outputs.long()[0]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([en_id2word[int(tok)] for tok in outputs[0][1:length]])\n",
        "    \n",
        "    else:\n",
        "        length = (outputs.long()[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([en_id2word[int(tok)] for tok in outputs[ind][1:length]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQqNaz7Mgg_b"
      },
      "source": [
        "def translate_sentence(sentence, model, opt):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    indexed = []\n",
        "    sentence = indic_tokenize.trivial_tokenize(sentence)\n",
        "    \n",
        "    for tok in sentence:\n",
        "        if tok in hi_word2id.keys():\n",
        "            if hi_word2id[tok] != 0:\n",
        "                indexed.append(hi_word2id[tok])\n",
        "        else:\n",
        "            indexed.append(hi_word2id['__<<unknown>>__'])\n",
        "\n",
        "    sentence = torch.Tensor([[0] + indexed + [1]]).long().to(device)\n",
        "\n",
        "    # print('SENT', sentence)\n",
        "\n",
        "    sentence = beam_search(sentence, model, opt)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zloUZ4H1JQ9n",
        "outputId": "d694995d-4e35-4e81-8fa1-4fbc2dfcc5fe"
      },
      "source": [
        "sent = \"मैं आज यहां भारत की जनता की संवेदना और प्रार्थना के साथ आया हूं ।\"\n",
        "\n",
        "print(sent in hi_dataset)\n",
        "\n",
        "translate_sentence(\"मैं आज यहां भारत की जनता की संवेदना और प्रार्थना के साथ आया हूं ।\", model, opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I have come here with a population of India ’ s population and with prayer .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u5VfEU1Lvg0"
      },
      "source": [
        "for i in range(20):\n",
        "    print('===========================================')\n",
        "    print(translate_sentence(dataset[i][0], model, opt))\n",
        "    print(dataset[i][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH6aT2c_p7ut"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}