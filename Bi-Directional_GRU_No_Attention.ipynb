{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi_Directional_GRU_No_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kopnIY-oqj1"
      },
      "source": [
        "# Check for GPU details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u_5mTgdZ4hs",
        "outputId": "decfd099-f837-4d78-c44f-7a998786e7ad"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tue Sep  7 08:54:27 2021       ',\n",
              " '+-----------------------------------------------------------------------------+',\n",
              " '| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |',\n",
              " '|-------------------------------+----------------------+----------------------+',\n",
              " '| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |',\n",
              " '| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |',\n",
              " '|                               |                      |               MIG M. |',\n",
              " '|===============================+======================+======================|',\n",
              " '|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |',\n",
              " '| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |',\n",
              " '|                               |                      |                  N/A |',\n",
              " '+-------------------------------+----------------------+----------------------+',\n",
              " '                                                                               ',\n",
              " '+-----------------------------------------------------------------------------+',\n",
              " '| Processes:                                                                  |',\n",
              " '|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |',\n",
              " '|        ID   ID                                                   Usage      |',\n",
              " '|=============================================================================|',\n",
              " '|  No running processes found                                                 |',\n",
              " '+-----------------------------------------------------------------------------+']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUeFW6fcqPXD"
      },
      "source": [
        "# **Preparing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOqIAcjIqe-V"
      },
      "source": [
        "### **Import of required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbtpKHzPs9mE"
      },
      "source": [
        "#Importing required libraries\n",
        "import random\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import re\n",
        "from numba import jit, cuda\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as Func\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT4dnf6CqqA-"
      },
      "source": [
        "### **Google Drive Mount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr-enVjLGksT",
        "outputId": "3d3c6a8c-4246-4206-ea6d-f52d0a6b06d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeZfnTU6XHwN",
        "outputId": "7c52a4b2-1e66-4df4-f210-faf9dc678bca"
      },
      "source": [
        "#Downloading and Cloning INDICNLP\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1325, done.\u001b[K\n",
            "remote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 1325 (delta 84), reused 89 (delta 41), pack-reused 1178\u001b[K\n",
            "Receiving objects: 100% (1325/1325), 9.57 MiB | 12.20 MiB/s, done.\n",
            "Resolving deltas: 100% (688/688), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 34.33 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Collecting Morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X2Ww6UmXNft"
      },
      "source": [
        "#Setting system path and environment variables\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "#Load IndicNLP library\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvKwzbN9yVg6"
      },
      "source": [
        "def remove_punctuations(text):\n",
        "\n",
        "    text=text.replace('-',' ')\n",
        "    text=text.replace('\"',' ')\n",
        "    text=text.replace('/',' ')\n",
        "    text=text.replace('~',' ')\n",
        "    text=text.replace('[',' ')\n",
        "    text=text.replace(']',' ')\n",
        "    text=text.replace('{',' ')\n",
        "    text=text.replace('}',' ')\n",
        "    text=text.replace('|',' ')\n",
        "    text=text.replace('।',' ')\n",
        "    text=re.sub('\\s+',' ',text)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6ixZb4jzzaZz",
        "outputId": "aaf37845-2a77-452f-803d-e835ac695f8e"
      },
      "source": [
        "remove_punctuations(\"THis is puctutation    testing    \")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'THis is puctutation testing '"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2y84QLDXKse"
      },
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "#Splitting a hindi paragraph into hindi sentences\n",
        "def SentenceSplitHindi(indic_string):\n",
        "    \n",
        "    indic_string = remove_punctuations(indic_string)\n",
        "    \n",
        "    sents = sentence_tokenize.sentence_split(indic_string, lang='hi')\n",
        "    sent1 = []\n",
        "    for s in sents:\n",
        "        #some special cases in sentences where \"()\" and \":\" were not splitted by indicnlp are splitted manually\n",
        "        if ')' in s:\n",
        "            splits = re.split(\"[?<=)]\", s)\n",
        "            sent1.append(splits[0]+')')\n",
        "            for sp in range(1, len(splits) - 1):\n",
        "                sent1.append(splits[sp][1:]+')')\n",
        "            if (len(splits[-1][1:]) > 0):\n",
        "                sent1.append(splits[-1][1:])\n",
        "        else:\n",
        "            sent1.append(s)\n",
        "\n",
        "    sentences = []\n",
        "    for s in sent1:\n",
        "        if ':' in s:\n",
        "            splits = re.split(\"[?<=:]\", s)\n",
        "            sentences.append(splits[0]+':')\n",
        "            for sp in range(1, len(splits) - 1):\n",
        "                sentences.append(splits[sp][1:]+':')\n",
        "\n",
        "            if (len(splits[-1][1:]) > 0):\n",
        "                sentences.append(splits[-1][1:])\n",
        "        else:\n",
        "            sentences.append(s)\n",
        "\n",
        "    return sentences      \n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eULHAfVXJVH"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize  \n",
        "\n",
        "#Tokenizing hindi sentences\n",
        "def TokenizeHindi(indic_string):\n",
        "    return indic_tokenize.trivial_tokenize(indic_string) \n",
        " "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KttgNHLa3Vh",
        "outputId": "d21a45dc-fd1a-4b59-c770-4e68b2d0a97c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Tokenizing English sentences\n",
        "def TokenizeEnglish(text):\n",
        "  return [token for token in word_tokenize(text)]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulg7yYKfd64g"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "#Splitting a English paragraph into english sentences\n",
        "def SentenceSplitEnglish(text):\n",
        "    text = remove_punctuations(text)\n",
        "    return [sent.strip() for sent in sent_tokenize(text)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9BP1SI8Bldg"
      },
      "source": [
        "### **Language Class**\n",
        "-------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD7Gt7oYs9mJ"
      },
      "source": [
        "SOS = 0 #Start of sentence\n",
        "EOS = 1 #End of sentence\n",
        "UNK = 2 #Unknown Word\n",
        "\n",
        "#Language class to store the words in the training data\n",
        "class Language:\n",
        "    def __init__(self, Name):\n",
        "        self.Name = Name\n",
        "        self.WordToIndex = {}\n",
        "        self.WordToCount = {}\n",
        "        self.IndexToWord = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.No_Of_Words = 3  # For SOS, EOS & UNK\n",
        " \n",
        "    def AddSentence(self, sentence):\n",
        "        if self.Name == \"Hindi\":\n",
        "            tokens = TokenizeHindi(sentence)\n",
        "        else:\n",
        "            tokens = TokenizeEnglish(sentence)\n",
        "\n",
        "        for word in tokens:\n",
        "            if word not in self.WordToIndex:\n",
        "                self.WordToIndex[word] = self.No_Of_Words\n",
        "                self.WordToCount[word] = 1\n",
        "                self.IndexToWord[self.No_Of_Words] = word\n",
        "                self.No_Of_Words += 1\n",
        "            else:\n",
        "                self.WordToCount[word] += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls2JrcHDBrmf"
      },
      "source": [
        "### **Getting Training Data**\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2yMjTEas9mL",
        "outputId": "a9553104-2109-4b52-b6d9-6344fef9e02d"
      },
      "source": [
        "#Reading training data and generating pairs from it.\n",
        "def ReadTrainingData(l1, l2):\n",
        "\n",
        "    lines = []\n",
        "    with open(\"drive/MyDrive/train.csv\", 'r') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "\n",
        "        for line in csvreader:\n",
        "            lines.append(line)\n",
        "    count = 0\n",
        "    pairs = []\n",
        "    #Generating pairs from training data\n",
        "    for l in lines[1:]:\n",
        "        hin_sents = SentenceSplitHindi(l[2])\n",
        "        eng_sents = SentenceSplitEnglish(l[1])\n",
        "        if (len(hin_sents)==len(eng_sents)):\n",
        "            for s in range(len(hin_sents)):\n",
        "                pairs.append([hin_sents[s], eng_sents[s]])\n",
        "\n",
        "        else:\n",
        "            count+=1 #Count for Discarded Pairs\n",
        "    #pairs = [[l[1], l[2]] for l in lines[1:]]\n",
        "\n",
        "    print(\"Discarded Pairs:\" + str(count))\n",
        "\n",
        "    #Creating language class objects for input and output languages\n",
        "    I_lang = Language(l1)\n",
        "    O_lang = Language(l2)\n",
        "\n",
        "    print(\"Training Pairs Available: %s\" % len(pairs)) #No. of training pairs available in training data\n",
        "\n",
        "    #Adding sentences from training data into the language class objects\n",
        "    for pair in pairs:\n",
        "        I_lang.AddSentence(pair[0])\n",
        "        O_lang.AddSentence(pair[1])\n",
        "    print(\"Word Count:\")\n",
        "    print(I_lang.Name + \" : \" + str(I_lang.No_Of_Words))\n",
        "    print(O_lang.Name + \" : \" + str(O_lang.No_Of_Words))\n",
        "\n",
        "    return I_lang, O_lang, pairs\n",
        "\n",
        "\n",
        "Input_lang, Output_lang, pairs = ReadTrainingData('Hindi', 'English')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discarded Pairs:20251\n",
            "Training Pairs Available: 110230\n",
            "Word Count:\n",
            "Hindi : 61541\n",
            "English : 62679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFgqydD4IYHI",
        "outputId": "15e2a963-34b6-425e-d322-b3a62a849f39"
      },
      "source": [
        "pairs[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .',\n",
              "  'politicians do not have permission to do what needs to be done.'],\n",
              " ['मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,',\n",
              "  \"I'd like to tell you about one such child,\"],\n",
              " ['यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है',\n",
              "  'This percentage is even greater than the percentage in India.'],\n",
              " ['हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते',\n",
              "  \"what we really mean is that they're bad at not paying attention.\"],\n",
              " ['इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है',\n",
              "  '.The ending portion of these Vedas is called Upanishad.'],\n",
              " ['कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का विरोध किया था , लेकिन अंग्रेजों की सहायता से उनकी आवाज दबा दी गयी .',\n",
              "  'The then Governor of Kashmir resisted transfer , but was finally reduced to subjection with the aid of British .'],\n",
              " ['इसमें तुमसे पूर्व गुज़रे हुए लोगों के हालात हैं',\n",
              "  'In this lies the circumstances of people before you.'],\n",
              " ['और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं',\n",
              "  'And who are we to say, even, that they are wrong'],\n",
              " ['ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों में हुई वार्मिंग और इसके निरंतर बने रहने के अनुमान और इसके अप्रत्यक्ष रूप से मानव पर पड़ने वाले प्रभाव से है',\n",
              "  '“”Global Warming“” refer to warming caused in recent decades and probability of its continual presence and its indirect effect on human being.'],\n",
              " ['हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटेन्ड ह्यबिना किसी समर्थन के हृ विशेष स्कूल , या किसी स्वतंत्र स्कूल में जाए , इजसके पास विशेष शैक्षणिक जऋऋरतों वाले बच्चों के प्रति सहूलियत हों . .',\n",
              "  \"You may want your child to go to a school that is not run by the LEA a non maintained special school or an independent school that can meet your child 's needs .\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa7vHDoXC-Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e73bfa2-a8cb-443a-b626-be706b3a46ca"
      },
      "source": [
        "def Find_Max(pairs):\n",
        "\n",
        "    max_hin = 0\n",
        "    max_eng = 0\n",
        "\n",
        "    #Finding Maximum length of sentences in hindi and english\n",
        "    for p in pairs:\n",
        "        hin = TokenizeHindi(p[0])\n",
        "        engl = TokenizeEnglish(p[1])\n",
        "        if len(hin) > max_hin:\n",
        "            max_hin = len(hin)\n",
        "\n",
        "        if len(engl) > max_eng:\n",
        "            max_eng = len(engl) \n",
        "\n",
        "\n",
        "    print(\"Maximum Sentece length in Hindi : \" + str(max_hin)) \n",
        "    print(\"Maximum Sentece length in English : \" + str(max_eng))   \n",
        "\n",
        "\n",
        "Find_Max(pairs)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Sentece length in Hindi : 194\n",
            "Maximum Sentece length in English : 192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo0byemiBZc6"
      },
      "source": [
        "Defining Neural Network Classes\n",
        "-------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogibg3BaEEjB"
      },
      "source": [
        "MAX_LENGTH = 175    #As seen above, Maximum sentence length  is less than 150, so I'd taken MAX_LENGTH = 150"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VcxkfiXs9mQ"
      },
      "source": [
        "\n",
        "#Encoder Class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, no_layers = 2, dropout = float(0.3)):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.no_layers = no_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.dropout = dropout\n",
        "        self.gru = nn.GRU(hidden_size, int(hidden_size/2) ,no_layers, dropout = dropout, bidirectional=True)\n",
        "\n",
        "    #Forward function for encoder class\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def InitializeHidden(self):\n",
        "        return torch.zeros(2*self.no_layers, 1, int(self.hidden_size/2), device=device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNlBmUIkh1eQ"
      },
      "source": [
        "\n",
        "#Decoder class\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, no_layers = 2, dropout = float(0.3)):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.no_layers = no_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, no_layers, dropout = dropout)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = Func.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.no_layers, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09t7Pz9Ps9mT"
      },
      "source": [
        "\n",
        "\n",
        "**Training**\n",
        "============="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzZySLAds9mU"
      },
      "source": [
        "#Generating Tensor for the input sentence\n",
        "def SentenceTensor(lang, sentence):\n",
        "    indexes = []\n",
        "    for word in TokenizeHindi(sentence):\n",
        "        if word not in lang.WordToIndex.keys():\n",
        "            indexes.append(2) #If a word is not in the corpus while running test data assign a default index(UNK)\n",
        "        else: \n",
        "            indexes.append(lang.WordToIndex[word])\n",
        "    indexes.append(EOS)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "#Generating pair of Tensors for the pair of sentences\n",
        "def PairTensor(pair):\n",
        "    InputTensor = SentenceTensor(Input_lang, pair[0])\n",
        "    TargetTensor = SentenceTensor(Output_lang, pair[1])\n",
        "    return (InputTensor, TargetTensor)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHKaISN0s9mU"
      },
      "source": [
        "Training the Model\n",
        "------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ4SdYcks9mV"
      },
      "source": [
        "TF_ratio = 0.5 # Teaching Forcing ratio parameter to apply teacher forcing randomly \n",
        "\n",
        "#Function to train one iteration for one pair of sentence\n",
        "def TrainOne(InputTensor, TargetTensor, encoder, decoder, encoder_optimizer, decoder_optimizer, LossCriteria, max_length=MAX_LENGTH):\n",
        "    en_hidden = encoder.InitializeHidden()\n",
        "\n",
        "    #intializing zero gradients for both the optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = InputTensor.size(0)\n",
        "    target_length = TargetTensor.size(0)\n",
        "\n",
        "    en_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #Calling encoder on the training instance\n",
        "    for en_index in range(input_length):\n",
        "        en_output, en_hidden = encoder(\n",
        "            InputTensor[en_index], en_hidden)\n",
        "        en_outputs[en_index] = en_output[0, 0]\n",
        "\n",
        "    de_input = torch.tensor([[SOS]], device=device)\n",
        "\n",
        "    bi_hidden = en_hidden.reshape(encoder.no_layers,1,encoder.hidden_size)\n",
        "    de_hidden = bi_hidden\n",
        "\n",
        "    #Taking a random input to decide that Teacher forcing will be used or not\n",
        "    TeacherForcing = True if random.random() < TF_ratio else False\n",
        "\n",
        "    if TeacherForcing:\n",
        "        # Teacher forcing: Feeding the Actual target(as in the training pair) as the next input\n",
        "        for de_index in range(target_length):\n",
        "            # de_output, de_hidden, de_attention = decoder(de_input, de_hidden, en_outputs)\n",
        "            de_output, de_hidden = decoder(de_input, de_hidden)\n",
        "\n",
        "            loss += LossCriteria(de_output, TargetTensor[de_index])\n",
        "            de_input = TargetTensor[de_index]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without Teacher forcing: Feeding its own predictions as the next input\n",
        "        for de_index in range(target_length):\n",
        "            # de_output, de_hidden, de_attention = decoder(de_input, de_hidden, en_outputs)\n",
        "            de_output, de_hidden = decoder(de_input, de_hidden)\n",
        "\n",
        "            topv, topi = de_output.topk(1)\n",
        "            de_input = topi.squeeze().detach()\n",
        "\n",
        "            loss += LossCriteria(de_output, TargetTensor[de_index])\n",
        "            if de_input.item() == EOS:\n",
        "                break\n",
        "\n",
        "    #Calling Backward function\n",
        "    loss.backward()\n",
        "\n",
        "    #Updating the optimizers\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAmbQ9l_s9mW"
      },
      "source": [
        "#Function to Print Time consumed \n",
        "def TimeInMinutes(since):\n",
        "    now = time.time()\n",
        "    sec = now - since\n",
        "    mins = math.floor(sec / 60)\n",
        "    sec -= mins * 60\n",
        "\n",
        "    return '%dmin %dsecs' % (mins, sec)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhteMkoLRv9e"
      },
      "source": [
        "#Function for plotting the losses\n",
        "def showPlot(points):\n",
        "    plt.plot(points)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pyStAcUs9mX"
      },
      "source": [
        "#Function for final training for all epochs\n",
        "def FinalTraining(encoder, decoder, no_iters, print_every=5000, plot_every=500, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    TotalPrintLoss = 0  # Reset every print_every\n",
        "    TotalPlotLoss = 0  # Reset every plot_every\n",
        "\n",
        "    #Average SGD instead of simple SGD\n",
        "    encoder_optimizer = optim.ASGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.ASGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    training_pairs = [PairTensor(random.choice(pairs)) for i in range(no_iters)]\n",
        "\n",
        "    #Poisson NLL Loss instead of Simple NLL Loss\n",
        "    LossCriteria = nn.NLLLoss()\n",
        "\n",
        "    for i in range(1, no_iters + 1):\n",
        "        training_pair = training_pairs[i - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        #Computing loss for single training input\n",
        "        loss = TrainOne(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, LossCriteria)\n",
        "        \n",
        "        TotalPrintLoss += loss\n",
        "        TotalPlotLoss += loss\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            \n",
        "            #Saving Encoder and Decoder models to Drive\n",
        "            torch.save(encoder, \"drive/MyDrive/Dissertation models/encoder5_0.pth\")\n",
        "            torch.save(decoder, \"drive/MyDrive/Dissertation models/decoder5_0.pth\")\n",
        "\n",
        "            print_loss_avg = TotalPrintLoss / print_every\n",
        "            TotalPrintLoss = 0\n",
        "            print('Time = %s (%d Iterations -- %d%%) Loss = %.4f' % (TimeInMinutes(start),\n",
        "                                         i, i / no_iters * 100, print_loss_avg))\n",
        "\n",
        "        if i % plot_every == 0:\n",
        "            plot_loss_avg = TotalPlotLoss / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            TotalPlotLoss = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao14eLYXGZOM"
      },
      "source": [
        "Calling FinalTraining() on every Training pair\n",
        "-------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0VKnUzPs9ma"
      },
      "source": [
        "#Initialize Training Model\n",
        "hidden_size = 300\n",
        "encoder = Encoder(Input_lang.No_Of_Words, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, Output_lang.No_Of_Words).to(device)\n",
        "\n",
        "#trained 50000 epochs\n",
        "FinalTraining(encoder, decoder, 50000, print_every=5000)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki_7LpsnPSpA"
      },
      "source": [
        "#Saving Encoder and Decoder models to Drive\n",
        "torch.save(encoder, \"drive/MyDrive/Dissertation models/encoder5_0.pth\")\n",
        "torch.save(decoder, \"drive/MyDrive/Dissertation models/decoder5_0.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRCkWvk0QcAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa28ec8-0988-4778-e583-1bf9d42a40fb"
      },
      "source": [
        "#Loading Encoder model from Drive\n",
        "encoder = torch.load(\"drive/MyDrive/Dissertation models/encoder5_0.pth\")\n",
        "encoder.eval()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoder(\n",
              "  (embedding): Embedding(61541, 300)\n",
              "  (gru): GRU(300, 150, num_layers=2, dropout=0.3, bidirectional=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmDCt25WQff-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa51b74-e9a3-4d42-d641-51ad46f6c730"
      },
      "source": [
        "#Loading Decoder model from Drive\n",
        "decoder = torch.load(\"drive/MyDrive/Dissertation models/decoder5_0.pth\")\n",
        "decoder.eval()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embedding): Embedding(62679, 300)\n",
              "  (gru): GRU(300, 300, num_layers=2, dropout=0.3)\n",
              "  (out): Linear(in_features=300, out_features=62679, bias=True)\n",
              "  (softmax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWasd196s9mZ"
      },
      "source": [
        "Translation \n",
        "-------------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eht8Ra0LPUqH"
      },
      "source": [
        "#Function for Translating one sentence from Hindi to English basically for test dataset\n",
        "def Translate_one(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = SentenceTensor(Input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        en_hidden = encoder.InitializeHidden()\n",
        "\n",
        "        en_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for en_index in range(input_length):\n",
        "            en_output, en_hidden = encoder(input_tensor[en_index], en_hidden)\n",
        "            en_outputs[en_index] += en_output[0, 0]\n",
        "\n",
        "        de_input = torch.tensor([[SOS]], device=device)  # SOS\n",
        "\n",
        "        bi_hidden = en_hidden.reshape(encoder.no_layers,1,encoder.hidden_size)        \n",
        "        de_hidden = bi_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        de_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for de_index in range(max_length):\n",
        "            de_output, de_hidden = decoder(de_input, de_hidden)\n",
        "            topv, topi = de_output.data.topk(1)\n",
        "            if topi.item() == EOS:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(Output_lang.IndexToWord[topi.item()])\n",
        "\n",
        "            de_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX0Oha4FGf3w"
      },
      "source": [
        "Translating Training dataset\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2x8vRubsmaT",
        "outputId": "d9d6135f-2693-44f1-fb15-bdc84df0e9ae"
      },
      "source": [
        "inp_sen = 'धन्यवाद'\n",
        "output_words = Translate_one(encoder, decoder, inp_sen)\n",
        "out_sen = ' '.join(output_words[:-1])\n",
        "print('Hindi: ', inp_sen)\n",
        "print('\\nEnglish: ', out_sen )"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi:  धन्यवाद\n",
            "\n",
            "English:  Thank you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIw80nQQWytb",
        "outputId": "9b59467e-3679-49f0-90ca-ce30e468a88b"
      },
      "source": [
        "inp_sen = 'आप कैसे हैं?'\n",
        "output_words = Translate_one(encoder, decoder, inp_sen)\n",
        "out_sen = ' '.join(output_words[:-1])\n",
        "print('Hindi: ', inp_sen)\n",
        "print('\\nEnglish: ', out_sen )"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi:  आप कैसे हैं?\n",
            "\n",
            "English:  How you ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68mqfobtXt7I",
        "outputId": "a37e2c03-fc89-4c64-c72e-a540c6cd1b13"
      },
      "source": [
        "inp_sen = 'तुमने क्या खाना बनाया है?'\n",
        "output_words = Translate_one(encoder, decoder, inp_sen)\n",
        "out_sen = ' '.join(output_words[:-1])\n",
        "print('Hindi: ', inp_sen)\n",
        "print('\\nEnglish: ', out_sen )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi:  तुमने क्या खाना बनाया है?\n",
            "\n",
            "English:  What you have the ? ? ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URZTtBPMifci"
      },
      "source": [
        "import io\n",
        "data_path = \"/content/drive/MyDrive/test_dataset.txt\"\n",
        "trgs = []\n",
        "src = []\n",
        "pred_trgs = []\n",
        "\n",
        "# Read in the data.\n",
        "lines = io.open(data_path, encoding = \"utf-8\").read().split(\"\\n\")\n",
        "lines  = lines[:-1]\n",
        "lines = [line.split(\"\\t\") for line in lines]\n",
        "src = [line[1] for line in lines]\n",
        "trgs = [line[0] for line in lines]\n",
        "\n",
        "for sent in src:\n",
        "  output_words = Translate_one(encoder, decoder, sent)\n",
        "  pred_trg = ' '.join(output_words[:-1])\n",
        "  pred_trgs.append(pred_trg)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6PgxLvCkJvF"
      },
      "source": [
        "pred_trgs = [pred_trg.split() for pred_trg in pred_trgs]\n",
        "trgs = [trg.split() for trg in trgs]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V8kTc5mkKV_",
        "outputId": "e7b9de3f-4f97-4543-890a-94a9fdcaf7f4"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "score = corpus_bleu(trgs, pred_trgs)\n",
        "print(score)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.036581411465600615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wbld0dzeXWJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}